services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    volumes:
      - namenode:/hadoop/dfs/name
      - ./config/hadoop:/hadoop-config
      - ./initdb/namenode-entrypoint.sh:/namenode-entrypoint.sh:ro
    entrypoint: [ "/bin/bash", "/namenode-entrypoint.sh" ]
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_DIR=/hadoop-config
      - HADOOP_CONF_DIR=/hadoop-config
    env_file:
      - ./config/hadoop-hive.env
    ports:
      - "9870:9870"
      - "8020:8020"
    healthcheck:
      test: [ "CMD", "curl", "-fsS", "http://localhost:9870/" ]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 10s
    networks:
      - hadoop-network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    hostname: datanode
    volumes:
      - datanode:/hadoop/dfs/data
      - ./config/hadoop:/hadoop-config
      - ./initdb/datanode-entrypoint.sh:/datanode-entrypoint.sh:ro
    entrypoint: [ "/bin/bash", "/datanode-entrypoint.sh" ]
    env_file:
      - ./config/hadoop-hive.env
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - SERVICE_PRECONDITION=namenode:9870
      - HDFS_CONF_DIR=/hadoop-config # <-- REQUIRED!!
      - HADOOP_CONF_DIR=/hadoop-config
    ports:
      - "9864:9864"
    depends_on:
      - namenode
    healthcheck:
      test: [ "CMD", "curl", "-fsS", "http://localhost:9864/" ]
      interval: 10s
      timeout: 5s
      retries: 8
      start_period: 10s
    networks:
      - hadoop-network

  zookeeper:
    image: zookeeper:3.5
    container_name: zookeeper
    ports:
      - "2181:2181"
    healthcheck:
      test: [ "CMD-SHELL", "echo ruok | nc 127.0.0.1 2181" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    volumes:
      - zookeeper_data:/data
      - zookeeper_datalog:/datalog
    networks:
      - hadoop-network

  hive-metastore-postgresql:
    image: postgres:12
    container_name: hive-metastore-postgresql
    environment:
      POSTGRES_USER: hiveuser
      POSTGRES_PASSWORD: hivepassword
      POSTGRES_DB: metastore
    volumes:
      - hive_metastore_db:/var/lib/postgresql/data
      - ./initdb/postgres:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U hiveuser -d metastore || exit 1" ]
      interval: 5s
      retries: 12
      timeout: 5s
      start_period: 5s
    networks:
      - hadoop-network

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./config/hadoop-hive.env
    volumes:
      - ./scripts/hive-startup.sh:/opt/hive-startup.sh
      - ./config/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
      - ./config//hadoop/core-site.xml:/opt/hive/conf/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/opt/hive/conf/hdfs-site.xml:ro
    command: [ "bash", "/opt/hive-startup.sh" ]
    depends_on:
      hive-metastore-postgresql:
        condition: service_healthy
      namenode:
        condition: service_healthy
      datanode:
        condition: service_healthy
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864 hive-metastore-postgresql:5432"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore-postgresql:5432/metastore"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionDriverName: "org.postgresql.Driver"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionUserName: "hiveuser"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionPassword: "hivepassword"
    ports:
      - "9083:9083"
    healthcheck:
      test: [ "CMD-SHELL", "nc -z hive-metastore 9083 || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 6
      start_period: 10s
    networks:
      - hadoop-network

  # update spark master depend on zooekeeper and hbase-master ( walid )
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master-new
    volumes:
      - ./config/hive-site.xml:/spark/conf/hive-site.xml
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./config/hadoop/core-site.xml:/etc/hadoop/core-site.xml
      - ./config/hadoop/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      - ./config/hadoop/core-site.xml:/spark/conf/core-site.xml
      - ./config/hadoop/hdfs-site.xml:/spark/conf/hdfs-site.xml
      - ./config/hbase-conf-dump/hbase-site.xml:/spark/conf/hbase-site.xml
      - ./spark_ml:/spark_ml
      - ./benchmark_results:/spark/reports

    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_CONF_DIR=/spark/conf
    env_file:
      - ./config/hadoop-hive.env
    depends_on:
      namenode:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
      hbase-master:
        condition: service_started
    networks:
      - hadoop-network

  spark-worker:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker
    volumes:
      - ./config/hive-site.xml:/spark/conf/hive-site.xml
      - ./config/hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./config/hbase-conf-dump/hbase-site.xml:/spark/conf/hbase-site.xml
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master-new:7077"
    env_file:
      - ./config/hadoop-hive.env
    networks:
      - hadoop-network

  hbase-master:
    image: harisekhon/hbase
    container_name: hbase-master
    hostname: hbase-master
    volumes:
      # Use /hbase/conf as the single HBase configuration directory used by the image.
      # Let the image manage hbase-site.xml internally to avoid sed -i on a bind mount.
      - ./config/hbase-conf-dump:/hbase/conf
      - ./config/hadoop/core-site.xml:/hbase/conf/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/hbase/conf/hdfs-site.xml:ro
    environment:
      HBASE_MODE: master
      HBASE_MASTER_HOST: hbase-master
      HBASE_ZOOKEEPER_QUORUM: zookeeper
      ZK_QUORUM: zookeeper
      HBASE_MANAGES_ZK: "false"
      # Align with the image defaults and our volume mounts
      HADOOP_CONF_DIR: /hbase/conf
      HBASE_CONF_DIR: /hbase/conf
      HBASE_ROOT_DIR: "hdfs://namenode:8020/hbase"
      # Correct Java path inside harisekhon/hbase image
      JAVA_HOME: /usr
      # Additional HBase configuration via environment
      HBASE_CLUSTER_DISTRIBUTED: "true"
      HBASE_ZOOKEEPER_PROPERTY_CLIENTPORT: "2181"
      ZOOKEEPER_ZNODE_PARENT: "/hbase"
    ports:
      - "16010:16010"
      - "16000:16000"
    depends_on:
      zookeeper:
        condition: service_healthy
      namenode:
        condition: service_healthy
      datanode:
        condition: service_healthy
      hdfs-init:
        condition: service_completed_successfully
    networks:
      - hadoop-network

  hbase-regionserver:
    image: harisekhon/hbase
    container_name: hbase-regionserver
    hostname: hbase-regionserver
    volumes:
      # Use the same single configuration directory as the master, but let the image manage hbase-site.xml.
      - ./config/hbase-conf-dump:/hbase/conf
      - ./config/hadoop/core-site.xml:/hbase/conf/core-site.xml:ro
      - ./config/hadoop/hdfs-site.xml:/hbase/conf/hdfs-site.xml:ro
    environment:
      HBASE_MODE: regionserver
      HBASE_MASTER_HOST: hbase-master
      HBASE_ZOOKEEPER_QUORUM: zookeeper
      ZK_QUORUM: zookeeper
      HBASE_MANAGES_ZK: "false"
      HADOOP_CONF_DIR: /hbase/conf
      HBASE_CONF_DIR: /hbase/conf
      HBASE_ROOT_DIR: "hdfs://namenode:8020/hbase"
      JAVA_HOME: /usr
      # Additional HBase configuration via environment
      HBASE_CLUSTER_DISTRIBUTED: "true"
      HBASE_ZOOKEEPER_PROPERTY_CLIENTPORT: "2181"
      ZOOKEEPER_ZNODE_PARENT: "/hbase"
    ports:
      - "16030:16030"
      - "16020:16020"
    depends_on:
      hbase-master:
        condition: service_started
      namenode:
        condition: service_healthy
      datanode:
        condition: service_healthy
      hdfs-init:
        condition: service_completed_successfully
    networks:
      - hadoop-network

  hdfs-init:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-init
    volumes:
      - namenode:/hadoop/dfs/name
      - datanode:/hadoop/dfs/data
      - ./initdb/hdfs-init.sh:/hdfs-init.sh:ro
    entrypoint: [ "/bin/bash", "-c", "/hdfs-init.sh" ]
    depends_on:
      namenode:
        condition: service_healthy
      datanode:
        condition: service_healthy
    restart: "no"
    networks:
      - hadoop-network

volumes:
  namenode:
  datanode:
  hive_metastore_db:
  hbase_data:
  zookeeper_data:
  zookeeper_datalog:


networks:
  hadoop-network:
    driver: bridge
